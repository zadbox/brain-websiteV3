#!/usr/bin/env python3
"""
Working RAG Server - Simplified version that actually works
Based on the successful simple_rag_test.py approach
"""
import sys
import os
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

import asyncio
import json
import time
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional

from fastapi import FastAPI, HTTPException, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# Import working components
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from config.settings import settings, CHROMA_DIR
from monitoring import get_metrics
from utils.session_manager import get_session_manager
from integrations.laravel_bridge import LaravelBridge

app = FastAPI(
    title="BrainGenTechnology Working RAG API",
    description="Working RAG system with Groq integration",
    version="1.0.0"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request/Response Models
class ChatRequest(BaseModel):
    message: str
    session_id: str
    metadata: Optional[Dict[str, Any]] = None

class ChatResponse(BaseModel):
    answer: str
    session_id: str
    sources: List[Dict[str, Any]] = []
    conversation_length: int
    timestamp: str
    processing_time: Optional[float] = None

# Global variables
groq_llm = None
vectorstore = None
conversations = {}
session_manager = None
laravel_bridge = None

def init_system():
    """Initialize the working system"""
    global groq_llm, vectorstore, session_manager, laravel_bridge
    
    print("üöÄ Initializing BrainGenTechnology Working RAG Server...")
    
    # Initialize Groq LLM
    try:
        groq_llm = ChatGroq(
            groq_api_key=settings.groq_api_key,
            model_name=settings.llm_model,
            temperature=0.3,
            max_tokens=1000
        )
        test_response = groq_llm.invoke("Say 'Groq ready' and nothing else.")
        print(f"‚úÖ Groq initialized: {test_response.content}")
    except Exception as e:
        print(f"‚ùå Groq initialization failed: {e}")
        groq_llm = None
    
    # Initialize vectorstore
    try:
        if (CHROMA_DIR / "chroma.sqlite3").exists():
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-mpnet-base-v2"
            )
            
            vectorstore = Chroma(
                persist_directory=str(CHROMA_DIR),
                embedding_function=embeddings,
                collection_name="braingentech_knowledge"
            )
            
            doc_count = vectorstore._collection.count()
            print(f"‚úÖ Vectorstore loaded with {doc_count} documents")
        else:
            print("‚ö†Ô∏è No vectorstore found")
            vectorstore = None
    except Exception as e:
        print(f"‚ùå Vectorstore initialization failed: {e}")
        vectorstore = None
    
    # Initialize Session Manager
    try:
        session_manager = get_session_manager()
        print("‚úÖ Session manager initialized")
    except Exception as e:
        print(f"‚ùå Session manager initialization failed: {e}")
        session_manager = None
    
    # Initialize Laravel Bridge
    try:
        laravel_bridge = LaravelBridge()
        print("‚úÖ Laravel bridge initialized")
    except Exception as e:
        print(f"‚ùå Laravel bridge initialization failed: {e}")
        laravel_bridge = None
    
    print(f"üìä System Status:")
    print(f"   Groq API: {'‚úÖ' if groq_llm else '‚ùå'}")
    print(f"   Vectorstore: {'‚úÖ' if vectorstore else '‚ùå'}")
    print(f"   Session Manager: {'‚úÖ' if session_manager else '‚ùå'}")
    print(f"   Laravel Bridge: {'‚úÖ' if laravel_bridge else '‚ùå'}")

def get_context_for_query(query: str) -> str:
    """Get relevant context from vectorstore"""
    try:
        if vectorstore is None:
            return ""
            
        docs = vectorstore.similarity_search(query, k=3)
        if not docs:
            return ""
            
        context = "\n\n".join([doc.page_content for doc in docs])
        return context[:2000]  # Limit context length
        
    except Exception as e:
        print(f"‚ö†Ô∏è Context retrieval failed: {e}")
        return ""

def create_rag_prompt(query: str, context: str = "") -> str:
    """Create a RAG prompt similar to the working test"""
    base_info = """You are a helpful AI assistant for BrainGenTechnology, a company that provides AI, automation, and blockchain solutions for businesses.

Company services include:
- AI Solutions: RAG systems, chatbots, machine learning, computer vision, predictive analytics
- Automation: RPA, workflow automation, business process optimization, document processing
- Blockchain: Supply chain traceability, smart contracts, digital identity, cryptocurrency integration

Company stats:
‚Ä¢ 500+ successful projects delivered
‚Ä¢ 98% client satisfaction rate  
‚Ä¢ Clients from startups to Fortune 500
‚Ä¢ 24/7 support available
‚Ä¢ ROI typically achieved within 6-12 months

Contact: contact@braingentech.com"""
    
    if context:
        prompt = f"""{base_info}

RELEVANT COMPANY INFORMATION:
{context}

Question: {query}

Please provide a helpful, professional response about BrainGenTechnology's services. Focus on business value and ask qualifying questions when appropriate. Keep responses concise but informative (2-3 paragraphs max)."""
    else:
        prompt = f"""{base_info}

Question: {query}

Please provide a helpful, professional response about BrainGenTechnology's services. Focus on business value and ask qualifying questions when appropriate."""
    
    return prompt

async def _check_and_create_consultation_request(qualification, session_id: str, conversation: list):
    """Check if lead is qualified and has contact info, then create consultation request"""
    try:
        # Only create consultation request if lead is sales ready
        if not qualification.sales_ready:
            print(f"üìã Lead {session_id} not sales ready (score: {qualification.lead_score})")
            return
        
        # Extract email and phone from conversation
        email, phone = _extract_contact_info(conversation)
        
        if not email or not phone:
            print(f"üìã Lead {session_id} is sales ready but missing contact info (email: {bool(email)}, phone: {bool(phone)})")
            return
        
        # Create consultation request
        if laravel_bridge:
            consultation_data = {
                'session_id': session_id,
                'email': email,
                'phone': phone,
                'preferred_contact': 'email',
                'preferred_time': 'flexible',
                'status': 'pending',
                'notes': f'BANT+ Score: {qualification.lead_score}/100 | Sales Ready: YES | '
                        f'Company: {getattr(qualification, "company_name", "N/A")} | '
                        f'Size: {getattr(qualification, "company_size", "N/A")} | '
                        f'Decision Level: {getattr(qualification, "authority_level", "N/A")} | '
                        f'Urgency: {getattr(qualification, "urgency", "medium")} | '
                        f'Tech Interests: {", ".join(getattr(qualification, "technology_interest", []))} | '
                        f'Pain Points: {getattr(qualification, "pain_points", "N/A")} | '
                        f'Generated automatically from BANT+ qualification',
                'industry': getattr(qualification, 'industry', 'unknown'),
                'request_type': 'consultation'
            }
            
            # Use Laravel bridge to create consultation request
            await _create_consultation_request_via_api(consultation_data)
            print(f"‚úÖ Created consultation request for qualified lead: {session_id}")
        else:
            print(f"‚ö†Ô∏è Laravel bridge not available for consultation request creation")
            
    except Exception as e:
        print(f"‚ùå Error creating consultation request for {session_id}: {e}")

def _extract_contact_info(conversation: list) -> tuple:
    """Extract email and phone from conversation messages"""
    import re
    
    email = None
    phone = None
    
    # Combine all user messages
    user_messages = [msg.get('content', '') for msg in conversation if msg.get('role') == 'user']
    full_text = ' '.join(user_messages).lower()
    
    # Email regex pattern
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    email_matches = re.findall(email_pattern, full_text, re.IGNORECASE)
    if email_matches:
        email = email_matches[0]
    
    # Phone regex patterns (various formats)
    phone_patterns = [
        r'\b\d{10}\b',  # 1234567890
        r'\b\d{3}[-.\s]?\d{3}[-.\s]?\d{4}\b',  # 123-456-7890, 123.456.7890, 123 456 7890
        r'\(\d{3}\)\s?\d{3}[-.\s]?\d{4}',  # (123) 456-7890
        r'\+\d{1,3}[-.\s]?\d{3,4}[-.\s]?\d{3,4}[-.\s]?\d{3,4}',  # International formats
    ]
    
    for pattern in phone_patterns:
        phone_matches = re.findall(pattern, full_text)
        if phone_matches:
            phone = phone_matches[0]
            break
    
    return email, phone

async def _create_consultation_request_via_api(consultation_data: dict):
    """Create consultation request via Laravel API"""
    try:
        import requests
        response = requests.post(
            f"{laravel_bridge.laravel_base_url}/api/consultation/request",
            json=consultation_data,
            timeout=5
        )
        
        if response.status_code == 200:
            print(f"‚úÖ Consultation request created successfully")
        else:
            print(f"‚ö†Ô∏è Laravel API error creating consultation request: {response.status_code}")
            
    except Exception as e:
        print(f"‚ùå Error calling Laravel API: {e}")

def create_enhanced_rag_prompt(query: str, context: str, user_context: str, conversation_history: str) -> str:
    """Create enhanced RAG prompt with user context and conversation history"""
    base_info = """You are an intelligent sales assistant for BrainGenTechnology, a leading provider of AI, automation, and blockchain solutions for enterprises.

COMPANY CONTEXT:
{context}

USER CONTEXT (REMEMBER THIS INFORMATION):
{user_context}

YOUR ROLE & OBJECTIVES:
- Engage professionally with international business prospects
- Remember and reference user information from previous messages (name, company, etc.)
- Provide accurate information using ONLY the provided context
- Identify business needs and pain points naturally through conversation
- Ask strategic qualifying questions to understand requirements
- Guide prospects toward appropriate solutions and next steps
- Focus on business value, ROI, and competitive advantages

CONVERSATION GUIDELINES:
- Maintain a professional yet approachable tone
- Always use the user's name when you know it
- Reference previous conversation topics and user details when relevant
- Use industry-standard terminology appropriately
- Emphasize measurable business benefits and success stories
- If information isn't in the context, clearly state limitations
- Suggest relevant case studies or solutions when appropriate
- Encourage next steps (consultation, demo, proposal)

QUALIFICATION FOCUS AREAS:
- Company size and industry vertical
- Current technology challenges and pain points
- Budget awareness and investment timeline
- Decision-making authority and process
- Specific use cases and requirements
- Urgency and implementation timeline

CONVERSATION HISTORY:
{conversation_history}

PROSPECT QUESTION: {query}

RESPONSE INSTRUCTIONS:
Provide a helpful, informative response that:
1. Directly addresses the prospect's question using context information
2. Uses the user's name and references their context when appropriate
3. Identifies potential business value or solutions
4. Naturally includes a qualifying question when appropriate
5. Suggests logical next steps or related topics
6. Maintains engagement while being genuinely helpful

Response:"""

    # Fill in the template
    prompt = base_info.format(
        context=context or "BrainGenTechnology offers comprehensive AI, automation, and blockchain solutions designed to drive business efficiency, accuracy, and profitability with a proven track record of 500+ successful projects and 98% client satisfaction.",
        user_context=user_context or "No previous user information available.",
        conversation_history=conversation_history or "This is the beginning of the conversation.",
        query=query
    )
    
    return prompt

async def process_chat_request(message: str, session_id: str) -> Dict[str, Any]:
    """Process chat request using working approach with session memory"""
    start_time = time.time()
    metrics = get_metrics()
    
    try:
        if groq_llm is None:
            metrics.record_error("llm_unavailable")
            return {
                "success": False,
                "answer": "AI system is initializing. Please try again in a moment.",
                "sources": [],
                "processing_time": time.time() - start_time
            }
        
        # Track message
        metrics.record_message("user")
        
        # Get context from vectorstore with timing
        vector_start = time.time()
        context = get_context_for_query(message)
        vector_duration = time.time() - vector_start
        metrics.record_vector_search(vector_duration)
        
        # Get user context from session manager
        user_context = ""
        conversation_history = ""
        if session_manager:
            try:
                user_context = session_manager.get_user_info_summary(session_id)
                # Get recent conversation history
                history = session_manager.get_conversation_history(session_id)
                if history:
                    # Format last few messages for context
                    recent_messages = history[-6:]  # Last 3 exchanges
                    conversation_history = "\n".join([
                        f"{'User' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
                        for msg in recent_messages
                    ])
            except Exception as e:
                print(f"Session context error: {e}")
        
        # Create enhanced prompt with user context
        prompt = create_enhanced_rag_prompt(message, context, user_context, conversation_history)
        
        # Get Groq response with timing
        llm_start = time.time()
        response = groq_llm.invoke(prompt)
        llm_duration = time.time() - llm_start
        
        # Track LLM request
        metrics.record_llm_request("success", settings.llm_model, llm_duration)
        
        # Estimate tokens (rough calculation)
        input_tokens = len(prompt.split()) * 1.3  # Approximate
        output_tokens = len(response.content.split()) * 1.3
        metrics.record_tokens(int(input_tokens), int(output_tokens))
        
        # Track assistant message
        metrics.record_message("assistant")
        
        # Prepare sources
        sources = []
        if context:
            sources.append({
                "source": "BrainGenTechnology Knowledge Base",
                "content": context[:200] + "..." if len(context) > 200 else context
            })
        
        processing_time = time.time() - start_time
        metrics.record_query("success", processing_time)
        
        # Store conversation in session manager
        if session_manager:
            try:
                session_manager.add_message_to_session(
                    session_id=session_id,
                    user_message=message,
                    ai_response=response.content,
                    metadata={"processing_time": processing_time, "sources": sources}
                )
            except Exception as e:
                print(f"Session storage error: {e}")
        
        return {
            "success": True,
            "answer": response.content,
            "sources": sources,
            "processing_time": processing_time
        }
        
    except Exception as e:
        print(f"‚ùå Chat processing error: {e}")
        import traceback
        traceback.print_exc()
        
        processing_time = time.time() - start_time
        return {
            "success": False,
            "answer": "I'm experiencing a technical issue. Please try again or contact our support team at contact@braingentech.com for assistance.",
            "sources": [],
            "processing_time": processing_time,
            "error": str(e)
        }

@app.on_event("startup")
async def startup_event():
    """Initialize components on startup"""
    init_system()

@app.get("/")
async def root():
    return {
        "message": "BrainGenTechnology Working RAG API",
        "version": "1.0.0",
        "status": "running"
    }

@app.get("/health")
async def health_check():
    global groq_llm, vectorstore
    
    return {
        "status": "healthy" if groq_llm is not None else "degraded",
        "version": "1.0.0",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "system_info": {
            "groq_available": groq_llm is not None,
            "vectorstore_available": vectorstore is not None,
            "llm_model": settings.llm_model,
            "document_count": vectorstore._collection.count() if vectorstore else 0
        }
    }

@app.get("/metrics")
async def metrics_endpoint():
    """Prometheus metrics endpoint"""
    metrics = get_metrics()
    
    # Update system metrics
    if vectorstore:
        try:
            doc_count = vectorstore._collection.count()
            metrics.update_vectorstore_size(doc_count)
        except:
            metrics.update_vectorstore_size(0)
    
    # Set system status
    metrics.rag_up.set(1 if groq_llm is not None else 0)
    
    # Return with proper Content-Type for Prometheus
    return Response(
        content=metrics.get_metrics(),
        media_type="text/plain; version=0.0.4; charset=utf-8"
    )

@app.post("/test-consultation")
async def test_consultation_creation():
    """Test endpoint to manually create a consultation request"""
    session_id = f"test_consultation_{int(time.time())}"
    
    # Create a test conversation with email and phone
    test_conversation = [
        {
            "role": "user",
            "content": "Hi, I'm Sarah Johnson, CEO of HealthTech Solutions. We need AI automation for our 150 employees. My email is sarah.johnson@healthtech.com and phone is 555-987-6543. We have budget of $200,000 and need to implement by Q4. Can we schedule a consultation?",
            "timestamp": datetime.now(timezone.utc).isoformat()
        },
        {
            "role": "assistant", 
            "content": "Hello Sarah! I'd be happy to help HealthTech Solutions with AI automation. Your needs sound like a perfect fit for our enterprise solutions.",
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    ]
    
    conversations[session_id] = test_conversation
    
    # Create a mock qualification object that will trigger consultation request
    class MockQualification:
        def __init__(self):
            self.sales_ready = True
            self.lead_score = 95
            self.company_name = "HealthTech Solutions"
            self.company_size = "sme"
            self.authority_level = "C_level"
            self.urgency = "high"
            self.technology_interest = ["ai", "automation"]
            self.pain_points = "Manual processes causing inefficiencies"
            self.industry = "healthcare"
    
    mock_qualification = MockQualification()
    
    # Test the consultation request creation
    await _check_and_create_consultation_request(mock_qualification, session_id, test_conversation)
    
    return {
        "success": True,
        "message": "Test consultation request created",
        "session_id": session_id,
        "qualification_score": mock_qualification.lead_score
    }

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    """Working chat endpoint"""
    
    # Track conversation
    if request.session_id not in conversations:
        conversations[request.session_id] = []
    
    conversations[request.session_id].append({
        "role": "user",
        "content": request.message,
        "timestamp": datetime.now(timezone.utc).isoformat()
    })
    
    # Store user message in Laravel
    if laravel_bridge:
        try:
            user_data = {
                'user_ip': request.metadata.get('user_ip', '127.0.0.1') if request.metadata else '127.0.0.1',
                'referrer': request.metadata.get('referrer', 'direct') if request.metadata else 'direct'
            }
            laravel_bridge.store_conversation(request.session_id, user_data)
            laravel_bridge.store_message(request.session_id, "user", request.message, request.metadata)
        except Exception as e:
            print(f"‚ö†Ô∏è Laravel bridge error (user message): {e}")
    
    # Process chat
    result = await process_chat_request(request.message, request.session_id)
    
    if result["success"]:
        conversations[request.session_id].append({
            "role": "assistant", 
            "content": result["answer"],
            "timestamp": datetime.now(timezone.utc).isoformat()
        })
        
        # Store assistant message in Laravel
        if laravel_bridge:
            try:
                laravel_bridge.store_message(request.session_id, "assistant", result["answer"], {
                    "processing_time": result.get("processing_time", 0),
                    "sources": result.get("sources", [])
                })
            except Exception as e:
                print(f"‚ö†Ô∏è Laravel bridge error (assistant message): {e}")
    
    return ChatResponse(
        answer=result["answer"],
        session_id=request.session_id,
        sources=result.get("sources", []),
        conversation_length=len(conversations[request.session_id]),
        timestamp=datetime.now(timezone.utc).isoformat(),
        processing_time=result.get("processing_time", 0)
    )

@app.post("/qualify")
async def qualify_lead(request: dict):
    """Advanced lead qualification using LLM analysis"""
    session_id = request.get("session_id", "unknown")
    conversation = conversations.get(session_id, [])
    
    if not conversation:
        return {
            "success": False,
            "error": "No conversation found for session",
            "session_id": session_id
        }
    
    try:
        # Import qualification chain
        from chains.qualification_chain import get_qualification_chain
        
        # Get qualification chain instance
        qualification_chain = get_qualification_chain()
        
        # Run qualification analysis with parser error catching
        try:
            result = await qualification_chain.qualify_lead(
                conversation_history=conversation,
                session_id=session_id
            )
            
            if result.success:
                # Convert to dict for JSON response
                qualification_dict = result.qualification.dict()
                
                # Check if lead is sales ready and has contact info
                await _check_and_create_consultation_request(result.qualification, session_id, conversation)
                
                return {
                    "success": True,
                    "qualification": qualification_dict,
                    "session_id": session_id,
                    "processing_time": result.processing_time
                }
            else:
                print(f"Qualification chain returned error: {result.error_message}")
                raise Exception(f"Chain error: {result.error_message}")
                
        except Exception as chain_error:
            print(f"Parser failed, using working_server fallback: {chain_error}")
            # Fall through to working_server fallback below
            raise chain_error
            
    except Exception as e:
        print(f"‚ùå Qualification error for session {session_id}: {e}")
        
        # Fallback to simple qualification if LLM fails
        from models.lead_qualification import LeadQualification
        
        score = min(len(conversation) * 15, 100)
        
        # Basic industry detection from conversation content
        conversation_text = ' '.join([msg.get('content', '') for msg in conversation]).lower()
        industry = "other"
        company_name = None
        
        # Simple keyword-based industry detection
        if any(word in conversation_text for word in ['bank', 'banking', 'financial', 'finance', 'fintech']):
            industry = "fintech"
            if 'wafae bank' in conversation_text:
                company_name = "Wafae Bank"
        elif any(word in conversation_text for word in ['health', 'medical', 'hospital', 'clinic']):
            industry = "healthcare"
        elif any(word in conversation_text for word in ['retail', 'store', 'shopping', 'ecommerce']):
            industry = "retail"
        elif any(word in conversation_text for word in ['manufacturing', 'factory', 'production']):
            industry = "manufacturing"
        elif any(word in conversation_text for word in ['tech', 'technology', 'software', 'startup']):
            industry = "other"
        
        qualification = LeadQualification(
            intent="consultation" if any(word in conversation_text for word in ['consultation', 'schedule', 'meeting']) else "information",
            urgency="high" if score >= 80 else "medium", 
            company_size="enterprise" if score >= 80 else "sme",
            industry=industry,
            company_name=company_name,
            lead_score=score,
            sales_ready=score >= 60,
            decision_maker_level="c_level" if any(word in conversation_text for word in ['ceo', 'cto', 'chief', 'director']) else "manager",
            next_action="schedule_call" if score >= 80 else "qualify_further",
            notes=f"Engaged in {len(conversation)} message conversation (fallback qualification with basic industry detection: {industry})",
            follow_up_priority="high" if score >= 80 else "medium",
            conversation_quality=min(10, max(1, len(conversation)))
        )
        
        # Check if lead is sales ready and has contact info
        await _check_and_create_consultation_request(qualification, session_id, conversation)
        
        return {
            "success": True,
            "qualification": qualification.dict(),
            "session_id": session_id,
            "processing_time": 0.5
        }

if __name__ == "__main__":
    import uvicorn
    
    print("üß† BrainGenTechnology Working RAG Server")
    print("=" * 50)
    
    uvicorn.run(app, host="0.0.0.0", port=8002, log_level="info")